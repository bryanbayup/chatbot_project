{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN1Rs46xgv0FgcWbl+fpjo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/chatbot_project/blob/main/finalisasi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalasi Library Tambahan\n",
        "!pip install gensim\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install imbalanced-learn\n",
        "!pip install Sastrawi\n",
        "!pip install sentencepiece\n",
        "!pip install seqeval\n",
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy1FZwE59O-q",
        "outputId": "f9617643-6e28-493f-8de2-fd039261b2b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (3.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import sentencepiece as spm\n",
        "from nltk.corpus import stopwords\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHUP4uFD9S9S",
        "outputId": "b0773270-8308-418f-ec40-06a0d9ef9d81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download FastText\n",
        "# !wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?dl=1\"\n",
        "# !tar -xzf id.tar.gz\n",
        "\n",
        "# Load FastText\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"Model FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    raise ValueError(\"Gagal memuat model FastText. Pastikan file 'id.vec' dalam format yang benar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5Om1un9WR9",
        "outputId": "5038f669-26b5-4642-c0cb-7835eb06b546"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FastText 'id.vec' berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from JSON file\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert dataset to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Ensure consistent column naming\n",
        "df.rename(columns={'utterance': 'utterances', 'response': 'responses'}, inplace=True)"
      ],
      "metadata": {
        "id": "e-Oe071h9alh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode intents\n",
        "label_encoder = LabelEncoder()\n",
        "df['intent_label'] = label_encoder.fit_transform(df['intent'])\n",
        "\n",
        "# Save intent mapping\n",
        "intent_mapping = dict(zip(df['intent_label'], df['intent']))\n",
        "\n",
        "# Handle imbalanced classes with oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df.index.values.reshape(-1, 1)\n",
        "y = df['intent_label']\n",
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "\n",
        "# Buat DataFrame baru dengan data yang telah di-oversample\n",
        "df_balanced = df.loc[X_ros.flatten()].reset_index(drop=True)\n",
        "df_balanced['intent_label'] = y_ros\n",
        "df_balanced['intent'] = label_encoder.inverse_transform(df_balanced['intent_label'])"
      ],
      "metadata": {
        "id": "QZWTn0P29dly"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Load custom stopwords\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(stop_words)\n",
        "\n",
        "# Stemming using Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Daftar kata yang dikenal dari data\n",
        "all_text = ' '.join(df_balanced['utterances'])\n",
        "tokenizer_vocab = set(all_text.split())\n",
        "\n",
        "def correct_typo(text):\n",
        "    tokens = text.split()\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in tokenizer_vocab:\n",
        "            matches = process.extractOne(token, tokenizer_vocab)\n",
        "            if matches and matches[1] > 80:  # Threshold kecocokan\n",
        "                corrected_tokens.append(matches[0])\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "    return ' '.join(corrected_tokens)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    text = correct_typo(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    text = stemmer.stem(text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df_balanced['utterances_clean'] = df_balanced['utterances'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "HYGOLdN-9gWP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare texts and labels\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Split data for Intent Classification\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Convert labels to categorical\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Create embedding matrix using FastText\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "-iesIHGK9ijt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data with entities\n",
        "df_ner = df[df['entities'].map(lambda d: len(d)) > 0].reset_index(drop=True)\n",
        "df_ner['utterances_clean'] = df_ner['utterances'].apply(preprocess_text)\n",
        "\n",
        "def prepare_ner_data(df, tokenizer, max_seq_length):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['utterances_clean']\n",
        "        entities = row['entities']\n",
        "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(tokens)\n",
        "        for ent in entities:\n",
        "            ent_text = preprocess_text(ent['value'])\n",
        "            ent_tokens = tokenizer.texts_to_sequences([ent_text])[0]\n",
        "            ent_len = len(ent_tokens)\n",
        "            for i in range(len(tokens) - ent_len + 1):\n",
        "                if tokens[i:i+ent_len] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, ent_len):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "        texts.append(tokens)\n",
        "        labels.append(label_seq)\n",
        "    # Padding\n",
        "    texts_padded = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
        "    # Padding labels\n",
        "    labels_padded = [label + ['O']*(max_seq_length - len(label)) for label in labels]\n",
        "    return texts_padded, labels_padded\n",
        "\n",
        "# Create label encoder for NER\n",
        "all_labels = set()\n",
        "for label_list in df_ner['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "all_labels.add('O')\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Prepare NER data\n",
        "texts_ner, labels_ner = prepare_ner_data(df_ner, tokenizer, max_seq_length)\n",
        "\n",
        "# Convert labels to numerical and categorical format\n",
        "def encode_ner_labels(labels, ner_label_encoder):\n",
        "    labels_encoded = []\n",
        "    for label_seq in labels:\n",
        "        label_ids = [ner_label_encoder[label] for label in label_seq]\n",
        "        labels_encoded.append(label_ids)\n",
        "    labels_encoded = np.array(labels_encoded)\n",
        "    labels_encoded = to_categorical(labels_encoded, num_classes=len(ner_label_encoder))\n",
        "    return labels_encoded\n",
        "\n",
        "labels_ner_encoded = encode_ner_labels(labels_ner, ner_label_encoder)\n",
        "\n",
        "# Split data for NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner,\n",
        "    labels_ner_encoded,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "gOaF2HSV9m84"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
        "    global_pool = GlobalMaxPooling1D()(conv)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(global_pool)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model_intent_cnn = build_intent_model_with_cnn(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001)\n",
        "model_intent_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_intent_cnn.summary()\n",
        "\n",
        "# Early stopping\n",
        "callbacks_intent = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history_intent_cnn = model_intent_cnn.fit(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_intent\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDZXtsnV9peu",
        "outputId": "c2abd28a-1249-49c8-f217-e38f5631de65"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 22)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 22, 300)           167700    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 20, 128)           115328    \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 128)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 52)                3380      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 294664 (1.12 MB)\n",
            "Trainable params: 294664 (1.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "150/150 [==============================] - 3s 14ms/step - loss: 3.0949 - accuracy: 0.2863 - val_loss: 1.8428 - val_accuracy: 0.7143\n",
            "Epoch 2/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 1.4809 - accuracy: 0.6785 - val_loss: 0.6695 - val_accuracy: 0.9060\n",
            "Epoch 3/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.8071 - accuracy: 0.8256 - val_loss: 0.3733 - val_accuracy: 0.9323\n",
            "Epoch 4/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.5598 - accuracy: 0.8889 - val_loss: 0.2893 - val_accuracy: 0.9398\n",
            "Epoch 5/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.4130 - accuracy: 0.9187 - val_loss: 0.2587 - val_accuracy: 0.9474\n",
            "Epoch 6/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.3293 - accuracy: 0.9405 - val_loss: 0.2218 - val_accuracy: 0.9624\n",
            "Epoch 7/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.3056 - accuracy: 0.9468 - val_loss: 0.2275 - val_accuracy: 0.9624\n",
            "Epoch 8/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.2542 - accuracy: 0.9640 - val_loss: 0.2219 - val_accuracy: 0.9511\n",
            "Epoch 9/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.2315 - accuracy: 0.9652 - val_loss: 0.2186 - val_accuracy: 0.9586\n",
            "Epoch 10/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.1998 - accuracy: 0.9744 - val_loss: 0.2181 - val_accuracy: 0.9662\n",
            "Epoch 11/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.1956 - accuracy: 0.9774 - val_loss: 0.2152 - val_accuracy: 0.9549\n",
            "Epoch 12/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.1752 - accuracy: 0.9790 - val_loss: 0.2235 - val_accuracy: 0.9586\n",
            "Epoch 13/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.1800 - accuracy: 0.9786 - val_loss: 0.2110 - val_accuracy: 0.9624\n",
            "Epoch 14/20\n",
            "150/150 [==============================] - 2s 11ms/step - loss: 0.1766 - accuracy: 0.9778 - val_loss: 0.2117 - val_accuracy: 0.9624\n",
            "Epoch 15/20\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.1592 - accuracy: 0.9799 - val_loss: 0.2120 - val_accuracy: 0.9662\n",
            "Epoch 16/20\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.1616 - accuracy: 0.9778 - val_loss: 0.1975 - val_accuracy: 0.9662\n",
            "Epoch 17/20\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.1596 - accuracy: 0.9786 - val_loss: 0.2219 - val_accuracy: 0.9624\n",
            "Epoch 18/20\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.1535 - accuracy: 0.9816 - val_loss: 0.2115 - val_accuracy: 0.9624\n",
            "Epoch 19/20\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.1385 - accuracy: 0.9866 - val_loss: 0.2016 - val_accuracy: 0.9699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NERHyperModel(HyperModel):\n",
        "    def __init__(self, embedding_matrix, max_seq_length, num_entities):\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_entities = num_entities\n",
        "\n",
        "    def build(self, hp):\n",
        "        l2_reg = hp.Choice('l2_reg', values=[1e-4, 1e-3, 1e-2])\n",
        "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "        lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "        inputs = Input(shape=(self.max_seq_length,))\n",
        "        embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.embedding_matrix.shape[0],\n",
        "            output_dim=self.embedding_matrix.shape[1],\n",
        "            weights=[self.embedding_matrix],\n",
        "            input_length=self.max_seq_length,\n",
        "            trainable=True\n",
        "        )(inputs)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, kernel_regularizer=l2(l2_reg), return_sequences=True))(embedding)\n",
        "        dropout = Dropout(dropout_rate)(lstm)\n",
        "        outputs = TimeDistributed(Dense(self.num_entities, activation='softmax'))(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# Initialize HyperModel\n",
        "ner_hypermodel = NERHyperModel(embedding_matrix, max_seq_length, len(ner_label_encoder))\n",
        "\n",
        "# Initialize RandomSearch\n",
        "tuner_ner = RandomSearch(\n",
        "    ner_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='ner_tuner_dir',\n",
        "    project_name='ner_tuning'\n",
        ")\n",
        "\n",
        "# Hyperparameter search for NER\n",
        "tuner_ner.search(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    epochs=10,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get the best model for NER\n",
        "best_model_ner = tuner_ner.get_best_models(num_models=1)[0]\n",
        "best_hp_ner = tuner_ner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters for NER: {best_hp_ner.values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvQnIddx9swk",
        "outputId": "6a6bff59-acde-4f34-b9ba-b63b8d9e5a79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 22s]\n",
            "val_accuracy: 0.9424715638160706\n",
            "\n",
            "Best val_accuracy So Far: 0.9438920617103577\n",
            "Total elapsed time: 00h 02m 34s\n",
            "Best Hyperparameters for NER: {'l2_reg': 0.0001, 'dropout_rate': 0.3, 'lstm_units': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Model Intent Classification\n",
        "loss_intent, accuracy_intent = model_intent_cnn.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy_intent * 100:.2f}%')\n",
        "\n",
        "# Evaluasi Model NER\n",
        "loss_ner, accuracy_ner = best_model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy_ner * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9WJDS1E93U4",
        "outputId": "a4d20fa2-ef6b-4326-f688-85a4e398623a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9662\n",
            "Akurasi Model Klasifikasi Intent: 96.62%\n",
            "2/2 [==============================] - 1s 12ms/step - loss: 0.2399 - accuracy: 0.9439\n",
            "Akurasi Model NER: 94.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories if not exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('encoders', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save intent model\n",
        "model_intent_cnn.save('models/model_intent.keras')\n",
        "\n",
        "# Save NER model\n",
        "best_model_ner.save('models/model_ner.keras')\n",
        "\n",
        "# Save tokenizer\n",
        "with open('encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save label encoder\n",
        "with open('encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save NER label encoder\n",
        "with open('encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Ig_Rk2zS9_w5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame for utterances and responses\n",
        "df_utterances = df_balanced[['utterances', 'responses', 'intent']].reset_index(drop=True)\n",
        "df_utterances['utterances_clean'] = df_utterances['utterances'].apply(preprocess_text)\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(df_utterances['utterances_clean'])\n",
        "\n",
        "# Save vectorizer\n",
        "with open('data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "o_UxveEW-Fem"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent_cnn.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = best_model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1], 'value': tokens[idx]})\n",
        "    return entities"
      ],
      "metadata": {
        "id": "ZLBNJwp8-Hre"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping antara intent dan hewan terkait\n",
        "intent_animal_mapping = {\n",
        "    'medical_inquiry_dog': 'anjing',\n",
        "    'medical_inquiry_cat': 'kucing',\n",
        "    'symptom_analysis_dog': 'anjing',\n",
        "    'symptom_analysis_cat': 'kucing',\n",
        "    'disease_prevention_dog': 'anjing',\n",
        "    'disease_prevention_cat': 'kucing',\n",
        "    'dog_healthcare': 'anjing',\n",
        "    'cat_healthcare': 'kucing',\n",
        "    'animal_health_issue': ['anjing', 'kucing'],\n",
        "    # Tambahkan mapping untuk intent lain yang relevan\n",
        "}\n",
        "\n",
        "# Fungsi untuk menyesuaikan intent berdasarkan entitas\n",
        "def adjust_intent(intent, entities):\n",
        "    # Dapatkan hewan dari intent yang diprediksi\n",
        "    predicted_animal = intent_animal_mapping.get(intent, None)\n",
        "\n",
        "    # Ekstrak entitas 'animal' dari input pengguna\n",
        "    entity_animals = [ent['value'].lower() for ent in entities if ent['entity'] == 'animal']\n",
        "\n",
        "    # Jika ada entitas 'animal' dalam input\n",
        "    if entity_animals:\n",
        "        user_animal = entity_animals[0]\n",
        "        # Jika hewan dari intent tidak sesuai dengan entitas 'animal' pengguna, sesuaikan intentnya\n",
        "        if predicted_animal:\n",
        "            if isinstance(predicted_animal, list):\n",
        "                if user_animal not in predicted_animal:\n",
        "                    intent = None\n",
        "            else:\n",
        "                if predicted_animal != user_animal:\n",
        "                    # Cari intent lain yang sesuai\n",
        "                    for intent_name, animal in intent_animal_mapping.items():\n",
        "                        if animal == user_animal and intent_name != intent:\n",
        "                            intent = intent_name\n",
        "                            break\n",
        "                    else:\n",
        "                        intent = None\n",
        "    else:\n",
        "        # Jika tidak ada entitas 'animal', dan intent membutuhkan hewan tertentu, set intent ke None\n",
        "        if intent in intent_animal_mapping:\n",
        "            intent = None\n",
        "    return intent\n",
        "\n",
        "# Fungsi untuk mendapatkan respon berdasarkan intent yang disesuaikan\n",
        "def get_response(user_input, intent=None, entities=None):\n",
        "    user_input_clean = preprocess_text(user_input)\n",
        "    print(f\"Input yang Dipreproses: {user_input_clean}\")\n",
        "\n",
        "    if intent:\n",
        "        # Filter dataset berdasarkan intent yang disesuaikan\n",
        "        df_intent = df_utterances[df_utterances['intent'] == intent]\n",
        "        if df_intent.empty:\n",
        "            print(\"Intent tidak ditemukan dalam dataset.\")\n",
        "            return get_default_response()\n",
        "        else:\n",
        "            # Vectorize ulang utterances yang difilter\n",
        "            tfidf_matrix_intent = vectorizer.transform(df_intent['utterances_clean'])\n",
        "            user_tfidf = vectorizer.transform([user_input_clean])\n",
        "            similarities = cosine_similarity(user_tfidf, tfidf_matrix_intent)\n",
        "            most_similar_idx = np.argmax(similarities[0])\n",
        "            highest_similarity = similarities[0][most_similar_idx]\n",
        "            print(f\"Kemiripan Tertinggi: {highest_similarity}\")\n",
        "            if highest_similarity < 0.2:  # Sesuaikan threshold sesuai kebutuhan\n",
        "                print(\"Kemiripan di bawah threshold.\")\n",
        "                return get_default_response()\n",
        "            else:\n",
        "                # Ambil respon yang sesuai dengan utterance paling mirip\n",
        "                response = df_intent.iloc[most_similar_idx]['responses']\n",
        "                print(f\"Respon yang Dipilih: {response}\")\n",
        "                return response\n",
        "    else:\n",
        "        print(\"Intent tidak tersedia.\")\n",
        "        return get_default_response()"
      ],
      "metadata": {
        "id": "OtgGue60-gUK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    # Prediksi intent dan entitas\n",
        "    intent = predict_intent(user_input)\n",
        "    entities = predict_entities(user_input)\n",
        "\n",
        "    # Sesuaikan intent berdasarkan entitas\n",
        "    adjusted_intent = adjust_intent(intent, entities)\n",
        "\n",
        "    print(f\"Intent yang Diprediksi: {intent}\")\n",
        "    print(f\"Entitas yang Diekstrak: {entities}\")\n",
        "    print(f\"Intent yang Disesuaikan: {adjusted_intent}\")\n",
        "\n",
        "    # Jika intent setelah disesuaikan adalah None, berikan respon default\n",
        "    if adjusted_intent is None:\n",
        "        response = get_default_response()\n",
        "    else:\n",
        "        # Dapatkan respon berdasarkan intent yang disesuaikan\n",
        "        response = get_response(user_input, adjusted_intent, entities)\n",
        "        # Jika tidak ada respon yang ditemukan, gunakan respon default\n",
        "        if not response:\n",
        "            response = get_default_response()\n",
        "    return response"
      ],
      "metadata": {
        "id": "ex81dOQt-ll6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mendapatkan respon default\n",
        "def get_default_response():\n",
        "    default_responses = [\n",
        "        \"Maaf, saya belum bisa menjawab pertanyaan Anda.\",\n",
        "        \"Maaf, mohon diperjelas apa yang Anda maksud.\",\n",
        "        \"Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\",\n",
        "        \"Mohon maaf, saya tidak mengerti. Bisa dijelaskan lebih detail?\",\n",
        "        \"Saya belum memiliki informasi mengenai hal tersebut.\"\n",
        "    ]\n",
        "    return random.choice(default_responses)"
      ],
      "metadata": {
        "id": "SIpyyFeo-ohj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Pengujian\n",
        "test_inputs = [\n",
        "    \"hai\",\n",
        "    \"halo selamat pagi\",\n",
        "    \"kucing saya muntah dan diare\",\n",
        "    \"anjing saya matanya bengkak\",\n",
        "    \"apa itu toxoplasmosis\",\n",
        "    \"AI itu apa\",\n",
        "    \"anjing tetangga suka menggonggong\",\n",
        "    \"Saya melihat seekor anjing tua tanpa kalung di kompleks. Apa yang harus saya lakukan?\",\n",
        "    \"Saya menemukan kucing dengan mata tertutup kotoran di depan pasar. Apa yang harus saya lakukan?\",\n",
        "    \"Apakah saya bisa membawa anjing ke dalam kereta api jarak jauh? Jika ya, apa yang harus disiapkan?\",\n",
        "    \"Saya ingin membawa kucing saya dalam perjalanan ke luar kota menggunakan pesawat. Apa saja yang perlu dipersiapkan?\",\n",
        "    \"Kucing saya seperti atlet parkour, suka melompat ke rak dapur dan menjatuhkan barang. Bagaimana saya bisa menghentikannya?\",\n",
        "    \"Saya melihat kucing di gang kecil yang terus mondar-mandir dengan ekspresi kebingungan\",\n",
        "    \"Ada seekor anjing besar yang tampak sakit di depan kantor saya\",\n",
        "    \"pasar itu apa\",\n",
        "    \"apa yang dimaksud dengan analisa fundamental\",\n",
        "    \"terimakasih\",\n",
        "    \"makasih ya\"\n",
        "]\n",
        "\n",
        "for input_text in test_inputs:\n",
        "    print(f\"Anda: {input_text}\")\n",
        "    response = chatbot_response(input_text)\n",
        "    print(f\"Chatbot: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF1MIojJ-rb5",
        "outputId": "2da78ec2-7feb-400c-8bbb-70ddb565974b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anda: hai\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: [{'entity': 'disease', 'value': 'hati'}]\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: hati\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Hai! Apa yang ingin Anda tanyakan?\n",
            "Chatbot: Hai! Apa yang ingin Anda tanyakan?\n",
            "\n",
            "Anda: halo selamat pagi\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: pagi\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Selamat pagi! Bagaimana kabar Anda hari ini?\n",
            "Chatbot: Selamat pagi! Bagaimana kabar Anda hari ini?\n",
            "\n",
            "Anda: kucing saya muntah dan diare\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Intent yang Diprediksi: cat_healthcare\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'symptom', 'value': 'muntah'}, {'entity': 'symptom', 'value': 'diare'}]\n",
            "Intent yang Disesuaikan: cat_healthcare\n",
            "Input yang Dipreproses: kucing muntah diare\n",
            "Kemiripan Tertinggi: 0.5629355754261958\n",
            "Respon yang Dipilih: Jika muntah hanya sekali dan kucing terlihat sehat, beri waktu istirahat sebelum makan berikutnya. Jika muntah terus-menerus, segera periksakan ke dokter hewan.\n",
            "Chatbot: Jika muntah hanya sekali dan kucing terlihat sehat, beri waktu istirahat sebelum makan berikutnya. Jika muntah terus-menerus, segera periksakan ke dokter hewan.\n",
            "\n",
            "Anda: anjing saya matanya bengkak\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Intent yang Diprediksi: symptom_analysis_dog\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'symptom', 'value': 'mata'}, {'entity': 'symptom', 'value': 'bengkak'}]\n",
            "Intent yang Disesuaikan: symptom_analysis_dog\n",
            "Input yang Dipreproses: anjing mata bengkak\n",
            "Kemiripan Tertinggi: 0.4155209191403108\n",
            "Respon yang Dipilih: Kesulitan membuka mata dan mata berair bisa menjadi tanda iritasi atau infeksi. Bersihkan dengan kain lembut dan konsultasikan ke dokter hewan.\n",
            "Chatbot: Kesulitan membuka mata dan mata berair bisa menjadi tanda iritasi atau infeksi. Bersihkan dengan kain lembut dan konsultasikan ke dokter hewan.\n",
            "\n",
            "Anda: apa itu toxoplasmosis\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: toxoplasmosis\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Mohon maaf, saya tidak mengerti. Bisa dijelaskan lebih detail?\n",
            "\n",
            "Anda: AI itu apa\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Intent yang Diprediksi: end_chat\n",
            "Entitas yang Diekstrak: [{'entity': 'disease', 'value': 'lain'}]\n",
            "Intent yang Disesuaikan: end_chat\n",
            "Input yang Dipreproses: lain\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\n",
            "\n",
            "Anda: anjing tetangga suka menggonggong\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: report_animal_noise\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}]\n",
            "Intent yang Disesuaikan: report_animal_noise\n",
            "Input yang Dipreproses: anjing tetangga suka gonggong\n",
            "Kemiripan Tertinggi: 0.4801319866742867\n",
            "Respon yang Dipilih: Coba bicarakan masalah ini dengan pemilik anjing. Jika tidak ada solusi, laporkan ke pengelola lingkungan.\n",
            "Chatbot: Coba bicarakan masalah ini dengan pemilik anjing. Jika tidak ada solusi, laporkan ke pengelola lingkungan.\n",
            "\n",
            "Anda: Saya melihat seekor anjing tua tanpa kalung di kompleks. Apa yang harus saya lakukan?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: found_stray_dog_street\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'animal', 'value': 'tua'}, {'entity': 'condition', 'value': 'kalung'}, {'entity': 'location', 'value': 'kompleks'}]\n",
            "Intent yang Disesuaikan: found_stray_dog_street\n",
            "Input yang Dipreproses: ekor anjing tua kalung kompleks laku\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Berikan air dan makanan, lalu coba cari informasi apakah ada pemilik yang kehilangan anjing.\n",
            "Chatbot: Berikan air dan makanan, lalu coba cari informasi apakah ada pemilik yang kehilangan anjing.\n",
            "\n",
            "Anda: Saya menemukan kucing dengan mata tertutup kotoran di depan pasar. Apa yang harus saya lakukan?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: found_cat_eye_problem\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'symptom', 'value': 'mata'}, {'entity': 'symptom', 'value': 'tutup'}, {'entity': 'symptom', 'value': 'kotor'}, {'entity': 'location', 'value': 'pasar'}]\n",
            "Intent yang Disesuaikan: found_cat_eye_problem\n",
            "Input yang Dipreproses: temu kucing mata tutup kotor pasar laku\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Bersihkan mata kucing dengan kapas lembut yang dibasahi air hangat, lalu bawa ke klinik hewan jika tidak membaik.\n",
            "Chatbot: Bersihkan mata kucing dengan kapas lembut yang dibasahi air hangat, lalu bawa ke klinik hewan jika tidak membaik.\n",
            "\n",
            "Anda: Apakah saya bisa membawa anjing ke dalam kereta api jarak jauh? Jika ya, apa yang harus disiapkan?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: pet_transportation\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'transport', 'value': 'kereta'}, {'entity': 'condition', 'value': 'api'}, {'entity': 'condition', 'value': 'jarak'}, {'entity': 'condition', 'value': 'jauh'}]\n",
            "Intent yang Disesuaikan: pet_transportation\n",
            "Input yang Dipreproses: bawa anjing kereta api jarak jauh ya siap\n",
            "Kemiripan Tertinggi: 1.0000000000000002\n",
            "Respon yang Dipilih: Hubungi operator kereta untuk memastikan kebijakan mereka. Siapkan dokumen vaksinasi dan kandang sesuai aturan.\n",
            "Chatbot: Hubungi operator kereta untuk memastikan kebijakan mereka. Siapkan dokumen vaksinasi dan kandang sesuai aturan.\n",
            "\n",
            "Anda: Saya ingin membawa kucing saya dalam perjalanan ke luar kota menggunakan pesawat. Apa saja yang perlu dipersiapkan?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: pet_transportation\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'location', 'value': 'jalan'}]\n",
            "Intent yang Disesuaikan: pet_transportation\n",
            "Input yang Dipreproses: bawa kucing jalan kota pesawat siap\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Pastikan kucing memiliki dokumen kesehatan, gunakan kandang yang sesuai, dan hubungi maskapai untuk informasi tambahan.\n",
            "Chatbot: Pastikan kucing memiliki dokumen kesehatan, gunakan kandang yang sesuai, dan hubungi maskapai untuk informasi tambahan.\n",
            "\n",
            "Anda: Kucing saya seperti atlet parkour, suka melompat ke rak dapur dan menjatuhkan barang. Bagaimana saya bisa menghentikannya?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: pet_behavior_issues\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'behavior', 'value': 'lompat'}, {'entity': 'behavior', 'value': 'rak'}, {'entity': 'behavior', 'value': 'dapur'}, {'entity': 'behavior', 'value': 'jatuh'}]\n",
            "Intent yang Disesuaikan: pet_behavior_issues\n",
            "Input yang Dipreproses: kucing atlet parkour suka lompat rak dapur jatuh barang henti\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Pastikan area dapur tidak menarik bagi kucing, seperti dengan menyimpan makanan di tempat tertutup. Berikan mainan atau tempat melompat alternatif.\n",
            "Chatbot: Pastikan area dapur tidak menarik bagi kucing, seperti dengan menyimpan makanan di tempat tertutup. Berikan mainan atau tempat melompat alternatif.\n",
            "\n",
            "Anda: Saya melihat kucing di gang kecil yang terus mondar-mandir dengan ekspresi kebingungan\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: found_confused_cat\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'kucing'}, {'entity': 'location', 'value': 'gang'}, {'entity': 'condition', 'value': 'mondar'}, {'entity': 'condition', 'value': 'mandir'}]\n",
            "Intent yang Disesuaikan: found_confused_cat\n",
            "Input yang Dipreproses: kucing gang mondar-mandir ekspresi bingung\n",
            "Kemiripan Tertinggi: 0.992802818801009\n",
            "Respon yang Dipilih: Dekati kucing tersebut secara perlahan. Jika memungkinkan, cek apakah kucing memiliki tanda pengenal.\n",
            "Chatbot: Dekati kucing tersebut secara perlahan. Jika memungkinkan, cek apakah kucing memiliki tanda pengenal.\n",
            "\n",
            "Anda: Ada seekor anjing besar yang tampak sakit di depan kantor saya\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Intent yang Diprediksi: found_animal\n",
            "Entitas yang Diekstrak: [{'entity': 'animal', 'value': 'anjing'}, {'entity': 'condition', 'value': 'sakit'}, {'entity': 'location', 'value': 'kantor'}]\n",
            "Intent yang Disesuaikan: found_animal\n",
            "Input yang Dipreproses: ekor anjing sakit kantor\n",
            "Kemiripan Tertinggi: 0.6804877951833141\n",
            "Respon yang Dipilih: Dekati anjing dengan hati-hati, berikan air atau makanan ringan, dan hubungi dokter hewan atau komunitas penyelamat.\n",
            "Chatbot: Dekati anjing dengan hati-hati, berikan air atau makanan ringan, dan hubungi dokter hewan atau komunitas penyelamat.\n",
            "\n",
            "Anda: pasar itu apa\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Intent yang Diprediksi: intro_chat\n",
            "Entitas yang Diekstrak: [{'entity': 'location', 'value': 'pasar'}]\n",
            "Intent yang Disesuaikan: intro_chat\n",
            "Input yang Dipreproses: pasar\n",
            "Kemiripan Tertinggi: 0.0\n",
            "Kemiripan di bawah threshold.\n",
            "Chatbot: Maaf, saya hanya diprogram untuk menjawab pertanyaan mengenai kucing dan anjing.\n",
            "\n",
            "Anda: apa yang dimaksud dengan analisa fundamental\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Intent yang Diprediksi: dog_healthcare\n",
            "Entitas yang Diekstrak: [{'entity': 'disease', 'value': 'mental'}]\n",
            "Intent yang Disesuaikan: None\n",
            "Chatbot: Mohon maaf, saya tidak mengerti. Bisa dijelaskan lebih detail?\n",
            "\n",
            "Anda: terimakasih\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: end_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: end_chat\n",
            "Input yang Dipreproses: terimakasih\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Sama-sama! Jika ada pertanyaan lain, jangan ragu untuk bertanya kembali.\n",
            "Chatbot: Sama-sama! Jika ada pertanyaan lain, jangan ragu untuk bertanya kembali.\n",
            "\n",
            "Anda: makasih ya\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Intent yang Diprediksi: end_chat\n",
            "Entitas yang Diekstrak: []\n",
            "Intent yang Disesuaikan: end_chat\n",
            "Input yang Dipreproses: terimakasih ya\n",
            "Kemiripan Tertinggi: 1.0\n",
            "Respon yang Dipilih: Sama-sama! Saya selalu siap membantu.\n",
            "Chatbot: Sama-sama! Saya selalu siap membantu.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeVFkVJj-vN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}