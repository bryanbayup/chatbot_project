{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHNPQDcVelGBZssgwCsVwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c9bce67a11147968bd3d339069a5639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "User Input:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_981c4b7c8c08434397b607ea8d447aef",
            "placeholder": "Ketik pertanyaan Anda di sini...",
            "style": "IPY_MODEL_c3d5fcc6ca2242ec965233c1748c531a",
            "value": ""
          }
        },
        "981c4b7c8c08434397b607ea8d447aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d5fcc6ca2242ec965233c1748c531a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb8696afc7984a63819deaa7a703ae18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Kirim",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_46658db663c04023a9f9198c5ae96ef8",
            "style": "IPY_MODEL_436af256e6924f5bb90fd2eda703d698",
            "tooltip": ""
          }
        },
        "46658db663c04023a9f9198c5ae96ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436af256e6924f5bb90fd2eda703d698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "aeb44e0e8e964d3bad14976966ad5708": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4e2c476d0f9e4149837d859a7914ff0c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "User: saya melihat kucing dijalan\n",
                  "Chatbot: Kucing ini mungkin kehilangan rumah. Anda dapat menggunakan fitur emergency untuk membantu mencarikan pemiliknya.\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "4e2c476d0f9e4149837d859a7914ff0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/chatbot_project/blob/main/multi_turn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim keras-tuner imbalanced-learn Sastrawi sentencepiece seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu_36DqYCW2O",
        "outputId": "4b1a0753-a900-42c6-9b0a-2661add1067a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1a141dab1a82d558e3bbef0c6b31839050a2f44ecb48b37467f7eca0b401e00c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: Sastrawi, kt-legacy, seqeval, keras-tuner\n",
            "Successfully installed Sastrawi-1.0.1 keras-tuner-1.4.7 kt-legacy-1.0.5 seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FastText\n",
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\"\n",
        "!tar -xzf id.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfaW2FTgMC1V",
        "outputId": "342b1521-1215-4cd0-f61d-55417438f975"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 03:00:10--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com/cd/0/inline/Cfx5j_BSCLKaecs7k71PY4OVK3b1eGOZsKcligQZ5uuypvaF9BKuK4FRYgCVIBgwlvXFvLe_I2uEyK0w6X4NSvZhRHwVaZA6Nyc9Afanggd5xLcgeD2JVpPzlR5wyEPENnI/file?dl=1# [following]\n",
            "--2024-12-07 03:00:11--  https://uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com/cd/0/inline/Cfx5j_BSCLKaecs7k71PY4OVK3b1eGOZsKcligQZ5uuypvaF9BKuK4FRYgCVIBgwlvXFvLe_I2uEyK0w6X4NSvZhRHwVaZA6Nyc9Afanggd5xLcgeD2JVpPzlR5wyEPENnI/file?dl=1\n",
            "Resolving uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com (uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com (uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cfz94nw2bOfzlvKiIq5muCsY4LpSzID7m8qcTPXpVwMv7NvpMv4BfPYsGRnYF2US4SDsnGDpjfoJkqa25nH1n3mdKwZnfOROS8fGe7RYFm_9ZjgpBk0hoCvhwGrNtHfXm26dPLOxVsEehyUq4V6S1y9dStP3J-f2HEe8ExYYKg8TY_3rORftDwPG28u0T_TEUnUx0ZA-ACJVkp4-WClC9j-jLNZwUAckS_fhixkYTlKUybN6jPLolszlPeoocxWRJOZOLoGlxdqNu0klc2GVUdK02EInFFQKgx3bgp8vOgOEqJbsaCQX1Bf-cHgDYTW5mAJPTwVVD3ZgZC0nJ2ys5aGbCEYOD8yGrrZnd8Z-Bn6OvQ/file?dl=1 [following]\n",
            "--2024-12-07 03:00:12--  https://uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com/cd/0/inline2/Cfz94nw2bOfzlvKiIq5muCsY4LpSzID7m8qcTPXpVwMv7NvpMv4BfPYsGRnYF2US4SDsnGDpjfoJkqa25nH1n3mdKwZnfOROS8fGe7RYFm_9ZjgpBk0hoCvhwGrNtHfXm26dPLOxVsEehyUq4V6S1y9dStP3J-f2HEe8ExYYKg8TY_3rORftDwPG28u0T_TEUnUx0ZA-ACJVkp4-WClC9j-jLNZwUAckS_fhixkYTlKUybN6jPLolszlPeoocxWRJOZOLoGlxdqNu0klc2GVUdK02EInFFQKgx3bgp8vOgOEqJbsaCQX1Bf-cHgDYTW5mAJPTwVVD3ZgZC0nJ2ys5aGbCEYOD8yGrrZnd8Z-Bn6OvQ/file?dl=1\n",
            "Reusing existing connection to uc3d0cf0333cda9c57fd3ece5f81.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  74.4MB/s    in 34s     \n",
            "\n",
            "2024-12-07 03:00:47 (64.6 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import (Dense, Input, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D,\n",
        "                                     TimeDistributed, Embedding, GlobalAveragePooling1D, Layer, Lambda)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# --------------------------------------\n",
        "# Load FastText\n",
        "# --------------------------------------\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    raise ValueError(\"Gagal memuat FastText.\")\n",
        "\n",
        "# --------------------------------------\n",
        "# Load Data\n",
        "# Pastikan 'data2.json' adalah file yang berisi dataset terakhir yang Anda berikan.\n",
        "# --------------------------------------\n",
        "with open('data2.json', 'r', encoding='utf-8') as f:\n",
        "    conversations = json.load(f)\n",
        "\n",
        "# --------------------------------------\n",
        "# Ekstrak Data\n",
        "# --------------------------------------\n",
        "def char_offset_to_token_labels(utterance, entities, tokenizer=lambda x: x.split()):\n",
        "    tokens = tokenizer(utterance)\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    char_pos = 0\n",
        "    token_ranges = []\n",
        "    for t in tokens:\n",
        "        start_pos = char_pos\n",
        "        end_pos = start_pos + len(t)\n",
        "        token_ranges.append((start_pos, end_pos))\n",
        "        char_pos = end_pos + 1\n",
        "    for ent in entities:\n",
        "        ent_start = ent['start']\n",
        "        ent_end = ent['end']\n",
        "        ent_type = ent['entity'].upper()\n",
        "        ent_token_positions = []\n",
        "        for i, (ts, te) in enumerate(token_ranges):\n",
        "            if not (te <= ent_start or ts >= ent_end):\n",
        "                ent_token_positions.append(i)\n",
        "        if len(ent_token_positions) > 0:\n",
        "            labels[ent_token_positions[0]] = \"B-\" + ent_type\n",
        "            for p in ent_token_positions[1:]:\n",
        "                labels[p] = \"I-\" + ent_type\n",
        "    return tokens, labels\n",
        "\n",
        "user_utterances = []\n",
        "intents = []\n",
        "entity_labels = []\n",
        "\n",
        "for conv in conversations:\n",
        "    for turn in conv[\"turns\"]:\n",
        "        if turn[\"speaker\"] == \"user\":\n",
        "            utt = turn[\"utterance\"]\n",
        "            ents = turn.get(\"entities\", [])\n",
        "            intent = turn.get(\"intent\", \"None\")\n",
        "            tokens, ner_tags = char_offset_to_token_labels(utt, ents)\n",
        "            user_utterances.append(tokens)\n",
        "            intents.append(intent)\n",
        "            entity_labels.append(ner_tags)\n",
        "\n",
        "# --------------------------------------\n",
        "# Preprocessing Text\n",
        "# --------------------------------------\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(word.strip().lower() for word in stop_words)\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "utterances_joined = [' '.join(utt) for utt in user_utterances]\n",
        "utterances_clean = [preprocess_text(u) for u in utterances_joined]\n",
        "\n",
        "df_data = pd.DataFrame({\n",
        "    'utterances': utterances_joined,\n",
        "    'intent': intents,\n",
        "    'entities': entity_labels,\n",
        "    'utterances_clean': utterances_clean\n",
        "})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_data['intent_label'] = label_encoder.fit_transform(df_data['intent'])\n",
        "\n",
        "# Balance data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X = df_data.index.values.reshape(-1, 1)\n",
        "y = df_data['intent_label']\n",
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "df_balanced = df_data.loc[X_ros.flatten()].reset_index(drop=True)\n",
        "df_balanced['intent_label'] = y_ros\n",
        "df_balanced['intent'] = label_encoder.inverse_transform(df_balanced['intent_label'])\n",
        "\n",
        "# --------------------------------------\n",
        "# Split Data for Intent Classification\n",
        "# --------------------------------------\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "embedding_dim = 300  # Gunakan embedding dim yang sama seperti fasttext\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "# --------------------------------------\n",
        "# NER Data\n",
        "# --------------------------------------\n",
        "all_labels = set()\n",
        "for tags in df_balanced['entities']:\n",
        "    for t in tags:\n",
        "        if t != 'O':\n",
        "            all_labels.add(t)\n",
        "all_labels.add('O')\n",
        "all_labels = sorted(list(all_labels))\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(all_labels)}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "def encode_tags(tags, max_len):\n",
        "    tag_ids = [ner_label_encoder[t] for t in tags]\n",
        "    tag_ids = tag_ids[:max_len] + [ner_label_encoder['O']]*(max_len - len(tag_ids))\n",
        "    return tag_ids\n",
        "\n",
        "def text_to_sequence(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    return seq[0]\n",
        "\n",
        "X_ner = []\n",
        "Y_ner = []\n",
        "for i, row in df_balanced.iterrows():\n",
        "    seq = text_to_sequence(row['utterances_clean'])\n",
        "    seq_padded = seq[:max_seq_length] + [0]*(max_seq_length - len(seq))\n",
        "    X_ner.append(seq_padded)\n",
        "    tag_ids = encode_tags(row['entities'], max_seq_length)\n",
        "    Y_ner.append(tag_ids)\n",
        "\n",
        "X_ner = np.array(X_ner)\n",
        "Y_ner = np.array(Y_ner)\n",
        "Y_ner = to_categorical(Y_ner, num_classes=len(ner_label_encoder))\n",
        "\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    X_ner,\n",
        "    Y_ner,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --------------------------------------\n",
        "# Attention Layer\n",
        "# --------------------------------------\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
        "                                 initializer='glorot_uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='att_bias', shape=(1,),\n",
        "                                 initializer='zeros', trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.squeeze(tf.tensordot(x, self.W, axes=1), axis=-1) + self.b\n",
        "        alpha = tf.nn.softmax(e)\n",
        "        alpha = tf.expand_dims(alpha, axis=-1)\n",
        "        context = x * alpha\n",
        "        return tf.reduce_sum(context, axis=1)  # shape: (batch, features)\n",
        "\n",
        "# --------------------------------------\n",
        "# Model Intent dengan BiLSTM + Attention\n",
        "# --------------------------------------\n",
        "def build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=1e-3):\n",
        "    inputs = Input(shape=(max_seq_length,), dtype='int32')\n",
        "    emb = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                    output_dim=embedding_matrix.shape[1],\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=max_seq_length,\n",
        "                    trainable=True, mask_zero=True)(inputs)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(emb)\n",
        "    att = AttentionLayer()(x)\n",
        "    dense = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(att)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_intent = build_intent_model(embedding_matrix, max_seq_length, num_classes)\n",
        "early_intent = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model_intent.fit(\n",
        "    train_padded, train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=30, batch_size=16,\n",
        "    callbacks=[early_intent]\n",
        ")\n",
        "\n",
        "# --------------------------------------\n",
        "# Model NER dengan BiLSTM + Attention\n",
        "# --------------------------------------\n",
        "def build_ner_model(embedding_matrix, max_seq_length, num_entities, l2_reg=1e-3):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    emb = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                    output_dim=embedding_matrix.shape[1],\n",
        "                    weights=[embedding_matrix],\n",
        "                    trainable=True)(inputs) # mask_zero dihapus\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(emb)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
        "\n",
        "    # Tetap gunakan TimeDistributed Dense untuk output NER\n",
        "    td = TimeDistributed(Dense(num_entities, activation='softmax'))(x)\n",
        "    model = Model(inputs, td)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_ner = build_ner_model(embedding_matrix, max_seq_length, len(ner_label_encoder))\n",
        "early_ner = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model_ner.fit(\n",
        "    train_texts_ner, train_labels_ner,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    epochs=30, batch_size=16,\n",
        "    callbacks=[early_ner]\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss_intent, acc_intent = model_intent.evaluate(val_padded, val_labels_cat)\n",
        "print(\"Akurasi Intent:\", acc_intent)\n",
        "\n",
        "loss_ner, acc_ner = model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(\"Akurasi NER:\", acc_ner)\n",
        "\n",
        "# --------------------------------------\n",
        "# Simpan Model\n",
        "# --------------------------------------\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('encoders', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "model_intent.save('models/model_intent.keras')\n",
        "model_ner.save('models/model_ner.keras')\n",
        "\n",
        "with open('encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "df_utterances = df_balanced[['utterances', 'intent', 'utterances_clean']].reset_index(drop=True)\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(df_utterances['utterances_clean'])\n",
        "with open('data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "intent_animal_mapping = {\n",
        "    \"Melaporkan Hewan Terlantar\": [\"kucing\", \"anjing\"],\n",
        "    \"Mendiagnosis Gejala\": [\"kucing\", \"anjing\"],\n",
        "    \"Rekomendasi Penanganan Awal\": [\"kucing\", \"anjing\"],\n",
        "    \"Konfirmasi Laporan\": [\"kucing\", \"anjing\"],\n",
        "    \"Tindak Lanjut Laporan\": [\"kucing\", \"anjing\"],\n",
        "    \"Rekomendasi Tindakan\": [\"kucing\", \"anjing\"]\n",
        "}\n",
        "\n",
        "def predict_intent(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_intent.predict(padded)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    return label_encoder.inverse_transform([predicted_label])[0]\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_ner.predict(padded)\n",
        "    pred_labels = np.argmax(pred[0], axis=-1)\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1].lower(), 'value': tokens[idx]})\n",
        "    return entities\n",
        "\n",
        "def adjust_intent(intent, entities):\n",
        "    predicted_animals = intent_animal_mapping.get(intent, None)\n",
        "    entity_animals = [ent['value'].lower() for ent in entities if ent['entity'] == 'animal']\n",
        "    if entity_animals and predicted_animals:\n",
        "        user_animal = entity_animals[0]\n",
        "        if user_animal not in predicted_animals:\n",
        "            for i_name, animals in intent_animal_mapping.items():\n",
        "                if user_animal in animals:\n",
        "                    intent = i_name\n",
        "                    break\n",
        "            else:\n",
        "                intent = None\n",
        "    return intent\n",
        "\n",
        "def get_default_response():\n",
        "    default_responses = [\n",
        "        \"Maaf, saya belum bisa menjawab pertanyaan Anda.\",\n",
        "        \"Mohon diperjelas, saya belum mengerti konteksnya.\",\n",
        "        \"Silakan berikan informasi lebih detail.\",\n",
        "        \"Maaf, saya hanya diprogram untuk menjawab mengenai kucing dan anjing.\",\n",
        "        \"Saya sarankan konsultasi langsung ke dokter hewan.\"\n",
        "    ]\n",
        "    return random.choice(default_responses)\n",
        "\n",
        "def get_response(user_input, intent=None, entities=None):\n",
        "    # Untuk kesederhanaan, kita berikan respon tergantung intent:\n",
        "    # Jika tidak puas dengan ini, Anda bisa membuat mapping intent ke response yang lebih baik.\n",
        "    if intent == \"Mendiagnosis Gejala\":\n",
        "        return \"Berdasarkan gejalanya, kemungkinan ada masalah kesehatan. Saya sarankan segera periksakan ke dokter hewan.\"\n",
        "    elif intent == \"Rekomendasi Penanganan Awal\":\n",
        "        return \"Cobalah langkah penanganan awal seperti menjaga kebersihan, memberikan makanan ringan, dan konsultasikan ke dokter hewan jika berlanjut.\"\n",
        "    else:\n",
        "        return get_default_response()\n",
        "\n",
        "dialog_history = []\n",
        "MAX_HISTORY = 2\n",
        "\n",
        "def predict_intent_multi_turn(dialog_history, current_input):\n",
        "    history_context = dialog_history[-MAX_HISTORY:] if len(dialog_history) > MAX_HISTORY else dialog_history\n",
        "    combined_input = \" \".join(history_context + [current_input])\n",
        "    return predict_intent(combined_input)\n",
        "\n",
        "def predict_entities_multi_turn(dialog_history, current_input):\n",
        "    history_context = dialog_history[-MAX_HISTORY:] if len(dialog_history) > MAX_HISTORY else dialog_history\n",
        "    combined_input = \" \".join(history_context + [current_input])\n",
        "    return predict_entities(combined_input)\n",
        "\n",
        "def chatbot_response_multi_turn(user_input):\n",
        "    dialog_history.append(user_input)\n",
        "    intent = predict_intent_multi_turn(dialog_history, user_input)\n",
        "    entities = predict_entities_multi_turn(dialog_history, user_input)\n",
        "    adjusted = adjust_intent(intent, entities)\n",
        "    if adjusted is None:\n",
        "        response = get_default_response()\n",
        "    else:\n",
        "        response = get_response(user_input, adjusted, entities)\n",
        "        if not response:\n",
        "            response = get_default_response()\n",
        "    return response\n",
        "\n",
        "# --------------------------------------\n",
        "# TESTING DENGAN PERTANYAAN YANG DIMINTA\n",
        "# Gunakan data \"conv_043\", \"conv_044\", \"conv_045\" yang telah Anda berikan.\n",
        "# --------------------------------------\n",
        "\n",
        "print(\"=== Test Model dengan Pertanyaan dari conv_043 ===\")\n",
        "dialog_history.clear()\n",
        "test_turns_043 = [\n",
        "    \"Saya melihat seekor kucing sakit di depan toko.\",\n",
        "    \"Kucingnya dalam keadaan sehat\"\n",
        "]\n",
        "\n",
        "for t in test_turns_043:\n",
        "    print(\"User:\", t)\n",
        "    resp = chatbot_response_multi_turn(t)\n",
        "    print(\"Chatbot:\", resp, \"\\n\")\n",
        "\n",
        "print(\"=== Test Model dengan Pertanyaan dari conv_044 ===\")\n",
        "dialog_history.clear()\n",
        "test_turns_044 = [\n",
        "    \"Anjing saya tidak nafsu makan dan sering diare. Apa yang terjadi?\",\n",
        "    \"Tidak, hanya diare saja.\"\n",
        "]\n",
        "\n",
        "for t in test_turns_044:\n",
        "    print(\"User:\", t)\n",
        "    resp = chatbot_response_multi_turn(t)\n",
        "    print(\"Chatbot:\", resp, \"\\n\")\n",
        "\n",
        "print(\"=== Test Model dengan Pertanyaan dari conv_045 ===\")\n",
        "dialog_history.clear()\n",
        "test_turns_045 = [\n",
        "    \"Kucing saya sering mengeluarkan air mata dan matanya terlihat merah.\",\n",
        "    \"Ya, ada renovasi rumah dengan banyak debu.\"\n",
        "]\n",
        "\n",
        "for t in test_turns_045:\n",
        "    print(\"User:\", t)\n",
        "    resp = chatbot_response_multi_turn(t)\n",
        "    print(\"Chatbot:\", resp, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTZbrFVvM3UZ",
        "outputId": "b1212c9d-effd-4907-824a-d6d431356628"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText 'id.vec' berhasil dimuat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'attention_layer' (of type AttentionLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 83ms/step - accuracy: 0.4038 - loss: 1.7261 - val_accuracy: 0.6286 - val_loss: 1.3319\n",
            "Epoch 2/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.6350 - loss: 1.2036 - val_accuracy: 0.8571 - val_loss: 0.7381\n",
            "Epoch 3/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.8654 - loss: 0.6800 - val_accuracy: 0.8429 - val_loss: 0.5018\n",
            "Epoch 4/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.8830 - loss: 0.4696 - val_accuracy: 0.9000 - val_loss: 0.3815\n",
            "Epoch 5/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9712 - loss: 0.2749 - val_accuracy: 0.8857 - val_loss: 0.3459\n",
            "Epoch 6/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9766 - loss: 0.2220 - val_accuracy: 0.9143 - val_loss: 0.3347\n",
            "Epoch 7/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9872 - loss: 0.1826 - val_accuracy: 0.9143 - val_loss: 0.3949\n",
            "Epoch 8/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9961 - loss: 0.1546 - val_accuracy: 0.9143 - val_loss: 0.3257\n",
            "Epoch 9/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9821 - loss: 0.1612 - val_accuracy: 0.9429 - val_loss: 0.3111\n",
            "Epoch 10/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.1264 - val_accuracy: 0.9286 - val_loss: 0.2678\n",
            "Epoch 11/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9829 - loss: 0.1675 - val_accuracy: 0.9143 - val_loss: 0.3721\n",
            "Epoch 12/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9824 - loss: 0.1369 - val_accuracy: 0.9571 - val_loss: 0.2358\n",
            "Epoch 13/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9990 - loss: 0.1117 - val_accuracy: 0.9571 - val_loss: 0.2281\n",
            "Epoch 14/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9985 - loss: 0.1095 - val_accuracy: 0.9429 - val_loss: 0.2238\n",
            "Epoch 15/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.9875 - loss: 0.1297 - val_accuracy: 0.9429 - val_loss: 0.2258\n",
            "Epoch 16/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9979 - loss: 0.0985 - val_accuracy: 0.9429 - val_loss: 0.2469\n",
            "Epoch 17/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9968 - loss: 0.0954 - val_accuracy: 0.9429 - val_loss: 0.2582\n",
            "Epoch 18/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9982 - loss: 0.0928 - val_accuracy: 0.9286 - val_loss: 0.2314\n",
            "Epoch 19/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0816 - val_accuracy: 0.9429 - val_loss: 0.2279\n",
            "Epoch 1/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 315ms/step - accuracy: 0.5679 - loss: 1.7889 - val_accuracy: 0.7429 - val_loss: 0.7503\n",
            "Epoch 2/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7330 - loss: 0.7448 - val_accuracy: 0.7714 - val_loss: 0.6416\n",
            "Epoch 3/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8051 - loss: 0.5448 - val_accuracy: 0.7841 - val_loss: 0.6461\n",
            "Epoch 4/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7891 - loss: 0.5288 - val_accuracy: 0.8063 - val_loss: 0.5559\n",
            "Epoch 5/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8250 - loss: 0.4630 - val_accuracy: 0.8238 - val_loss: 0.4721\n",
            "Epoch 6/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8236 - loss: 0.4690 - val_accuracy: 0.8381 - val_loss: 0.5012\n",
            "Epoch 7/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.8504 - loss: 0.3868 - val_accuracy: 0.8381 - val_loss: 0.4615\n",
            "Epoch 8/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8606 - loss: 0.3778 - val_accuracy: 0.8492 - val_loss: 0.4507\n",
            "Epoch 9/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8552 - loss: 0.3689 - val_accuracy: 0.8476 - val_loss: 0.4228\n",
            "Epoch 10/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8646 - loss: 0.3550 - val_accuracy: 0.8524 - val_loss: 0.4452\n",
            "Epoch 11/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8731 - loss: 0.3192 - val_accuracy: 0.8651 - val_loss: 0.4344\n",
            "Epoch 12/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8848 - loss: 0.3160 - val_accuracy: 0.8444 - val_loss: 0.4602\n",
            "Epoch 13/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8887 - loss: 0.3011 - val_accuracy: 0.8698 - val_loss: 0.4087\n",
            "Epoch 14/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9022 - loss: 0.2687 - val_accuracy: 0.8698 - val_loss: 0.4330\n",
            "Epoch 15/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8905 - loss: 0.2841 - val_accuracy: 0.8587 - val_loss: 0.4923\n",
            "Epoch 16/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9051 - loss: 0.2646 - val_accuracy: 0.8873 - val_loss: 0.3981\n",
            "Epoch 17/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9092 - loss: 0.2393 - val_accuracy: 0.8810 - val_loss: 0.3969\n",
            "Epoch 18/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9030 - loss: 0.2455 - val_accuracy: 0.8778 - val_loss: 0.4195\n",
            "Epoch 19/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9186 - loss: 0.2227 - val_accuracy: 0.8921 - val_loss: 0.4180\n",
            "Epoch 20/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.9230 - loss: 0.2005 - val_accuracy: 0.8921 - val_loss: 0.4269\n",
            "Epoch 21/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9324 - loss: 0.1858 - val_accuracy: 0.8857 - val_loss: 0.4603\n",
            "Epoch 22/30\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9321 - loss: 0.1821 - val_accuracy: 0.8889 - val_loss: 0.4195\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9246 - loss: 0.2502\n",
            "Akurasi Intent: 0.9428571462631226\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8754 - loss: 0.4271\n",
            "Akurasi NER: 0.8809524178504944\n",
            "=== Test Model dengan Pertanyaan dari conv_043 ===\n",
            "User: Saya melihat seekor kucing sakit di depan toko.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'attention_layer' (of type AttentionLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Chatbot: Saya sarankan konsultasi langsung ke dokter hewan. \n",
            "\n",
            "User: Kucingnya dalam keadaan sehat\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Chatbot: Mohon diperjelas, saya belum mengerti konteksnya. \n",
            "\n",
            "=== Test Model dengan Pertanyaan dari conv_044 ===\n",
            "User: Anjing saya tidak nafsu makan dan sering diare. Apa yang terjadi?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Chatbot: Berdasarkan gejalanya, kemungkinan ada masalah kesehatan. Saya sarankan segera periksakan ke dokter hewan. \n",
            "\n",
            "User: Tidak, hanya diare saja.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Chatbot: Saya sarankan konsultasi langsung ke dokter hewan. \n",
            "\n",
            "=== Test Model dengan Pertanyaan dari conv_045 ===\n",
            "User: Kucing saya sering mengeluarkan air mata dan matanya terlihat merah.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Chatbot: Berdasarkan gejalanya, kemungkinan ada masalah kesehatan. Saya sarankan segera periksakan ke dokter hewan. \n",
            "\n",
            "User: Ya, ada renovasi rumah dengan banyak debu.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Chatbot: Cobalah langkah penanganan awal seperti menjaga kebersihan, memberikan makanan ringan, dan konsultasikan ke dokter hewan jika berlanjut. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Pastikan file data sudah diupload, misalnya data2.json\n",
        "with open('data2.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(w.strip().lower() for w in stop_words)\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Kita akan membangun knowledge base berupa list dari (user_utterance_clean, bot_response)\n",
        "# Caranya: Untuk setiap conversation, kita iterate turns.\n",
        "# Setiap kali menemukan turn user, kita cek turn berikutnya apakah bot ada respons\n",
        "# Jika ya, simpan (user_utterance_clean, bot_utterance) ke knowledge base\n",
        "\n",
        "knowledge_base = []\n",
        "for conv in data:\n",
        "    turns = conv[\"turns\"]\n",
        "    for i in range(len(turns)-1):\n",
        "        if turns[i][\"speaker\"] == \"user\" and turns[i+1][\"speaker\"] == \"bot\":\n",
        "            user_utt = turns[i][\"utterance\"]\n",
        "            bot_utt = turns[i+1][\"utterance\"]\n",
        "            user_utt_clean = preprocess_text(user_utt)\n",
        "            knowledge_base.append((user_utt_clean, bot_utt))\n",
        "\n",
        "# Sekarang kita punya daftar pasangan (user_utterance_clean, bot_response)\n",
        "# Buat DataFrame untuk memudahkan\n",
        "df_kb = pd.DataFrame(knowledge_base, columns=[\"user_clean\",\"bot_response\"])\n",
        "\n",
        "# Vectorize user_utterance_clean untuk similarity\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_kb[\"user_clean\"])\n",
        "\n",
        "def get_best_response(user_input):\n",
        "    # Preprocess query\n",
        "    user_query_clean = preprocess_text(user_input)\n",
        "    q_vec = vectorizer.transform([user_query_clean])\n",
        "    sims = cosine_similarity(q_vec, X)  # (1, n)\n",
        "    best_idx = sims[0].argmax()\n",
        "    best_score = sims[0][best_idx]\n",
        "    # Threshold opsional, misal jika similarity < 0.2 balas default\n",
        "    if best_score < 0.2:\n",
        "        return \"Maaf, saya belum memiliki informasi yang sesuai.\"\n",
        "    else:\n",
        "        return df_kb[\"bot_response\"].iloc[best_idx]\n",
        "\n",
        "# Contoh pengujian\n",
        "test_input = \"Kucing saya sering batuk dan hidungnya berair. Apa yang salah?\"\n",
        "response = get_best_response(test_input)\n",
        "print(\"User:\", test_input)\n",
        "print(\"Chatbot:\", response)\n",
        "\n",
        "# Anda juga dapat menguji dengan input lain yang mirip dengan dataset\n",
        "test_input2 = \"Anjing saya tidak mau makan dan dia mencret\"\n",
        "response2 = get_best_response(test_input2)\n",
        "print(\"User:\", test_input2)\n",
        "print(\"Chatbot:\", response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHTFcW6oOS-f",
        "outputId": "8950884a-04e9-40b3-bb20-4e7ea8de881c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Kucing saya sering batuk dan hidungnya berair. Apa yang salah?\n",
            "Chatbot: Gejala ini dapat menunjukkan Infeksi Saluran Pernapasan Atas. Apakah kucing Anda sering terpapar udara dingin atau debu?\n",
            "User: Anjing saya tidak mau makan dan dia mencret\n",
            "Chatbot: Jika tetap tidak mau makan selama lebih dari 24 jam, segera bawa anjing Anda ke dokter hewan untuk pemeriksaan lebih lanjut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Kita asumsikan fungsi get_best_response dan variabel pendukung sudah didefinisikan sebelumnya:\n",
        "# get_best_response(user_input) -> string\n",
        "\n",
        "# Jika Anda ingin memasukkan konteks multi-turn, misalnya menggabungkan dua turn terakhir:\n",
        "dialog_history = []\n",
        "MAX_HISTORY = 2  # banyaknya turn sebelumnya yang akan digabung\n",
        "\n",
        "def combined_context(new_input):\n",
        "    # Menggabungkan turn terakhir di dialog_history dengan current input\n",
        "    # untuk mempertahankan konteks multi-turn.\n",
        "    history_context = dialog_history[-MAX_HISTORY:] if len(dialog_history) > MAX_HISTORY else dialog_history\n",
        "    combined = \" \".join(history_context + [new_input])\n",
        "    return combined\n",
        "\n",
        "# Input widget\n",
        "input_box = widgets.Text(\n",
        "    placeholder='Ketik pertanyaan Anda di sini...',\n",
        "    description='User Input:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "send_button = widgets.Button(\n",
        "    description='Kirim',\n",
        "    disabled=False,\n",
        "    button_style='success'\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_send_clicked(b):\n",
        "    user_input = input_box.value.strip()\n",
        "    if user_input:\n",
        "        dialog_history.append(user_input)\n",
        "        # Buat query dengan konteks multi-turn\n",
        "        query_with_context = combined_context(user_input)\n",
        "        response = get_best_response(query_with_context)\n",
        "        with output_area:\n",
        "            print(f\"User: {user_input}\")\n",
        "            print(f\"Chatbot: {response}\\n\")\n",
        "    input_box.value = \"\"  # kosongkan setelah kirim\n",
        "\n",
        "send_button.on_click(on_send_clicked)\n",
        "\n",
        "display(input_box, send_button, output_area)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "8c9bce67a11147968bd3d339069a5639",
            "981c4b7c8c08434397b607ea8d447aef",
            "c3d5fcc6ca2242ec965233c1748c531a",
            "bb8696afc7984a63819deaa7a703ae18",
            "46658db663c04023a9f9198c5ae96ef8",
            "436af256e6924f5bb90fd2eda703d698",
            "aeb44e0e8e964d3bad14976966ad5708",
            "4e2c476d0f9e4149837d859a7914ff0c"
          ]
        },
        "id": "E8PLVn8lUuol",
        "outputId": "4668a2f6-b48c-4c3e-81e3-b559dbffa378"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='User Input:', placeholder='Ketik pertanyaan Anda di sini...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c9bce67a11147968bd3d339069a5639"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Kirim', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb8696afc7984a63819deaa7a703ae18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeb44e0e8e964d3bad14976966ad5708"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hzc-7_DlVBqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}